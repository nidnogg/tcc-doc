\chapter{INTRODUÇÃO}

Para o contexto de desenvolvimento de software, segurança da informação é a área que potencialmente define entre o sucesso ou fracasso de uma aplicação. E em aplicações web com elevado tráfego, a preocupação com a segurança pode ter uma manifestação tardia e comprometer o uso do ambiente. Atualmente navegadores web como \textit{Chrome}, \textit{Firefox} e \textit{Safari} ativamente notificam o usuário de falhas graves nesse quesito como certificação HTTPS, tornando ações fundamentais como transações (quando inseguras) na web uma área de atenção redobrada para consumidores finais. Nela, ações de segurança são um recurso crítico e sensível que faz com que poucos sites discorram sobre os seus segredos de segurança, até como uma estratégia contra \textit{hackers}. Isso torna complicado assegurar como é o estado atual de vulnerabilidades web hoje em dia. Nesse sentido, este trabalho procura elencar o estado atual das medidas conhecidas para conter o avanço de ataques às aplicações web e introduzir uma nova ferramenta \textit{open-source} como contribuição adicional para fortalecer equipes de segurança contra as vulnerabilidades associadas aos sítios web. 

Historicamente, o conceito de vulnerabilidade na Internet modificou-se consideravelmente. A \textit{World Wide Web} consistia, nos seus primórdios, de web sites cujas funções principais eram essencialmente fornecer ao usuário final documentos estáticos de informação, surgida inicialmente nos anos 90. O fluxo de informação era de via única, no qual o conteúdo saía do servidor ao usuário final, apenas. Autenticação não era uma realidade para a maioria dos sites, pois não havia necessidade. Cada usuário que visitasse uma página web recebia a mesma informação. \cite{stuttard_web_nodate} Vulnerabilidades nessa época consistiam principalmente em numerosos \textit{exploits} em cima de software de servidores, para distribuir \textit{warez} \footnote{warez: Software de computador ilegalmente copiado com seus códigos de proteção desativados. A desativação dos códigos de proteção é importante para que a cópia possa ser distribuída sem dificuldades.} ou meramente deturpar a parte visual de um site (prática conhecida como \textit{defacing}).

Na atualidade a maioria dos sites na Internet são aplicações robustas com diversas funcionalidades que eram inconcebíveis na época, dependendo de um fluxo de informação de via dupla entre cliente e servidor. Usuários precisam de credenciais para realizar cadastros que cuidam das mais modestas trocas de informações como postagens em fóruns de discussão até os mais sensíveis dados bancários transacionais. Surge a necessidade concreta de gerenciadores de senha de ponta, muitas vezes oferecidos pelo próprio navegador web (embora inseguro). Além disso, torna-se onipresente a linguagem de script/programação \textit{JavaScript} moderna, que fornece a essas aplicações um dinamismo inerente, tanto na parte visual do cliente (\textit{front-end}), como na parte funcional do servidor (\textit{back-end}, através de tecnologias como Node.js). Cada internauta torna-se diferente do próximo, tornando a interação com o site totalmente dependente do usuário, o que afeta o conteúdo que é acessado a cada instante.

O uso maciço da internet pela população traz inúmeras possibilidades de ataques maliciosos, com exploração constante de novas técnicas aproveitando as vulnerabilidades inerentes ao ambiente compartilhado da internet. Um tipo de componente amplamente usado para mitigar vulnerabilidades em uma dada aplicação web é um \textit{Web Application Firewall} - responsável por monitorar tráfego de internet que é conduzido pela aplicação, permitindo ou bloqueando pacotes que possam ser potencialmente maliciosos. E recentemente, com o advento de técnicas de Aprendizado de Máquina, uma série de \textit{Web Application Firewalls} baseados nesse subcampo de Inteligência Artificial foram criados com o intuito não apenas de explorar a área em si, como também de gerar \textit{Firewalls} mais robustos e capazes de deter tráfego de ataques web. 

\textit{Firewalls} propriamente ditos são softwares ou até mesmo hardwares que são responsáveis por examinar e filtrar a informação que atravessa uma determinada conexão. \textit{Web Application Firewalls} (WAFs) monitoram dados que uma aplicação web recebe, permitindo averiguar suas vulnerabilidades. Há várias maneiras de testar WAFs hoje quanto a eficácia, tanto manualmente através de documentos padronizados como o \textit{wafec} \cite{wafec_doc}, e também de ferramentas como o \textit{GoTestWAF} \cite{gotestwaf_wallarm}. Com a sofisticação de tais \textit{firewalls}, surge um modelo novo dentro do paradigma de Aprendizado de Máquina - os WAFs que usam técnicas desse subcampo de Inteligência Artificial para realizar a etapa de filtragem da informação. E nem toda ferramenta de teste é capaz de avaliar e/ou realçar a eficácia desses.

Uma das ferramentas responsáveis por isso, no entanto, é o \textit{WAF-A-MoLE} \cite{valenza_waf--mole_2020}  que permite o uso de técnicas de \textit{fuzzing} \cite{fuzzing_book} para explorar as fraquezas de um WAF e com isso apontar caminhos para tornar os ambientes mais robustos. Ele traz um ambiente interessante e eficaz, mas tem uma arquitetura engessada que dificulta o uso e reprodução da ferramenta com um WAF próprio, e com a utilização de modelos de ataque (classificadores treinados) próprios. 

Não introduziu nada sobre os firewalls e entrou de sola com o wafamole. Deveria antecipar dizendo que há ferramentas para testar os firewalls embutidos nos sites e averiguar suas fragilidades. Ciitar algumas ou dizer que o \textit{WAF-A-MoLE} \cite{valenza_waf--mole_2020} é uma delas e que permite o uso de técnicas de \textit{fuzzing} \cite{fuzzing_book} para explorar as fraquezas e com isso apontar caminhos para tornar os ambientes mais robustos.

Essa trabalho procurou complementar o \textit{WAF-A-MoLE} em uma série de falhas verificadas pelos autores - uma arquitetura engessada que dificultava o uso e reprodução da ferramenta, uma série de modelos de ataque faltantes que poderiam ser utilizados porém não foram, e uma lista de operadores de mutação (um aspecto chave do seu funcionamento, explicado mais adiante) que poderia ser ainda mais estendida. Para mitigar essas dificuldades de desenvolvimento e uso, foram criadas várias ferramentas auxiliares que possibilitam a um contribuidor ou usuário treinar (e testar) um \textit{Web Application Firewall} baseado em aprendizado de máquina mais facilmente, um \textit{wrapper} para qualquer classificador da biblioteca \verb+scikit-learn+ fora introduzido, e a eficiência do programa foi incrementada com novos operadores de mutação.

Nesse contexto, o trabalho está organizado em seis capítulos. 
\begin{alineas}
\item No primeiro capítulo foi apresentada a introdução e motivação do mesmo; \item Em seguida, um apanhado de conceitos básicos é feito para a melhor compreensão das ferramentas, tecnologias e vulnerabilidades discutidas;
\item Posteriormente, uma revisão de literatura sobre a área e trabalhos relacionados é amostrada, traçando comparações com a ferramenta que foi escolhida como alvo de extensão/implementação do trabalho;
\item Um detalhamento sobre a implementação, arquitetura e conceitos por trás da extensão do \textit{WAF-A-MoLE}, intitulada de \textit{wafamole++};
\item Um compilado de experimentos responsáveis pela avaliação do \textit{wafamole++}, e seus resultados;
\item Finalmente, as conclusões acerca dos resultados, dificuldades encontradas e anexos.
\end{alineas}

\bigskip
